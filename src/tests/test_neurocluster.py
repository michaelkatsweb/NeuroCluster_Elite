#!/usr/bin/env python3
"""
File: test_neurocluster.py
Path: NeuroCluster-Elite/src/tests/test_neurocluster.py
Description: Comprehensive tests for NeuroCluster Elite core algorithm

This module contains extensive tests for the NeuroCluster algorithm including:
- Unit tests for core functionality
- Performance benchmarks (0.045ms target)
- Accuracy validation (94.7% target)
- Memory usage verification (12.4MB target)
- Regime detection testing
- Feature extraction validation
- Pattern recognition accuracy

The tests validate the proven 99.59% efficiency and ensure
the algorithm maintains its performance characteristics.

Author: Your Name
Created: 2025-06-30
Version: 1.0.0
License: MIT
"""

import pytest
import numpy as np
import pandas as pd
import time
import psutil
import asyncio
from typing import Dict, List, Tuple, Any
from datetime import datetime, timedelta
from unittest.mock import Mock, patch
import logging

# Import test utilities
from . import (
    TestConfig, TEST_CONFIG, TestDataGenerator, MockDataProvider,
    AsyncTestCase, CustomAssertions, benchmark, PERFORMANCE_MONITOR
)

# Import the core NeuroCluster module
try:
    from src.core.neurocluster_elite import (
        NeuroClusterElite, RegimeType, AssetType, MarketData,
        PerformanceMetrics, ClusteringResult
    )
    from src.core.feature_extractor import FeatureExtractor
    from src.core.pattern_recognition import PatternRecognition
    from src.core.regime_detector import RegimeDetector
except ImportError as e:
    pytest.skip(f"NeuroCluster modules not available: {e}", allow_module_level=True)

# Configure logging for tests
logging.getLogger('src.core').setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# ==================== TEST FIXTURES ====================

@pytest.fixture(scope="class")
def neurocluster_instance():
    """Create NeuroCluster instance for testing"""
    config = {
        'performance_mode': True,
        'enable_caching': True,
        'max_memory_mb': 50,  # Test with limited memory
        'target_processing_time_ms': 0.045
    }
    return NeuroClusterElite(config)

@pytest.fixture(scope="function")
def sample_market_data():
    """Generate sample market data for testing"""
    return TestDataGenerator.generate_market_data(
        symbol="AAPL",
        start_date=datetime.now() - timedelta(days=7),
        end_date=datetime.now(),
        frequency="1min"
    )

@pytest.fixture(scope="function")
def multi_asset_data():
    """Generate multi-asset market data"""
    symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'BTC/USD', 'ETH/USD']
    data = {}
    
    for symbol in symbols:
        asset_type = AssetType.CRYPTO if '/' in symbol else AssetType.STOCK
        data[symbol] = {
            'data': TestDataGenerator.generate_market_data(symbol=symbol),
            'asset_type': asset_type
        }
    
    return data

@pytest.fixture(scope="function")
def performance_baseline():
    """Establish performance baseline metrics"""
    return {
        'max_processing_time_ms': 0.045,
        'target_accuracy': 94.7,
        'max_memory_mb': 12.4,
        'target_efficiency': 99.59
    }

# ==================== CORE ALGORITHM TESTS ====================

class TestNeuroClusterCore:
    """Test core NeuroCluster functionality"""
    
    def test_neurocluster_initialization(self, neurocluster_instance):
        """Test NeuroCluster initialization"""
        nc = neurocluster_instance
        
        assert nc is not None
        assert nc.config is not None
        assert nc.performance_metrics is not None
        assert nc.feature_extractor is not None
        assert nc.pattern_recognition is not None
        assert nc.regime_detector is not None
        
        # Check initial state
        assert nc.is_initialized == True
        assert nc.total_samples_processed == 0
        assert nc.current_efficiency >= 0
        
        logger.info("âœ… NeuroCluster initialization test passed")
    
    @benchmark(threshold=0.001)  # 1ms for basic processing
    def test_process_single_datapoint(self, neurocluster_instance):
        """Test processing single data point performance"""
        nc = neurocluster_instance
        
        # Create single market data point
        market_data = MarketData(
            symbol="AAPL",
            asset_type=AssetType.STOCK,
            price=150.50,
            change=1.25,
            change_percent=0.83,
            volume=1000000,
            timestamp=datetime.now(),
            rsi=65.0,
            macd=0.5,
            macd_signal=0.3,
            volatility=15.2
        )
        
        start_time = time.perf_counter()
        
        # Process data point
        result = nc.process_datapoint(market_data)
        
        end_time = time.perf_counter()
        processing_time = (end_time - start_time) * 1000  # Convert to milliseconds
        
        # Validate result
        assert result is not None
        assert hasattr(result, 'regime')
        assert hasattr(result, 'confidence')
        assert hasattr(result, 'features')
        
        # Validate performance
        assert processing_time <= 0.045, f"Processing time {processing_time:.3f}ms exceeds target 0.045ms"
        
        # Validate confidence
        assert 0 <= result.confidence <= 100, f"Confidence {result.confidence} not in valid range"
        
        logger.info(f"âœ… Single datapoint processing: {processing_time:.3f}ms (target: 0.045ms)")
    
    @benchmark(threshold=0.5)  # 500ms for batch processing
    def test_process_batch_data(self, neurocluster_instance, sample_market_data):
        """Test batch data processing performance"""
        nc = neurocluster_instance
        
        # Convert DataFrame to MarketData objects
        market_data_list = []
        for _, row in sample_market_data.head(1000).iterrows():  # Test with 1000 points
            market_data = MarketData(
                symbol="AAPL",
                asset_type=AssetType.STOCK,
                price=row['close'],
                change=row['close'] - row['open'],
                change_percent=((row['close'] - row['open']) / row['open']) * 100,
                volume=int(row['volume']),
                timestamp=row.name,
                rsi=np.random.uniform(30, 70),  # Mock technical indicators
                macd=np.random.uniform(-1, 1),
                volatility=np.random.uniform(10, 25)
            )
            market_data_list.append(market_data)
        
        start_time = time.perf_counter()
        
        # Process batch
        results = nc.process_batch(market_data_list)
        
        end_time = time.perf_counter()
        total_time = end_time - start_time
        avg_time_per_point = (total_time * 1000) / len(market_data_list)  # ms per point
        
        # Validate results
        assert len(results) == len(market_data_list)
        assert all(r is not None for r in results)
        
        # Validate performance
        assert avg_time_per_point <= 0.045, f"Average processing time {avg_time_per_point:.3f}ms exceeds target"
        
        # Validate efficiency
        efficiency = nc.get_current_efficiency()
        assert efficiency >= 95.0, f"Efficiency {efficiency:.2f}% below acceptable threshold"
        
        logger.info(f"âœ… Batch processing: {avg_time_per_point:.3f}ms/point, efficiency: {efficiency:.2f}%")
    
    def test_memory_usage(self, neurocluster_instance, sample_market_data):
        """Test memory usage compliance"""
        nc = neurocluster_instance
        
        # Measure baseline memory
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        # Process substantial amount of data
        market_data_list = []
        for _, row in sample_market_data.head(5000).iterrows():  # Large dataset
            market_data = MarketData(
                symbol="AAPL",
                asset_type=AssetType.STOCK,
                price=row['close'],
                change=row['close'] - row['open'],
                change_percent=((row['close'] - row['open']) / row['open']) * 100,
                volume=int(row['volume']),
                timestamp=row.name,
                rsi=np.random.uniform(30, 70),
                volatility=np.random.uniform(10, 25)
            )
            market_data_list.append(market_data)
        
        # Process data
        results = nc.process_batch(market_data_list)
        
        # Measure memory after processing
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_delta = memory_after - memory_before
        
        # Validate memory usage
        assert memory_delta <= 12.4, f"Memory usage {memory_delta:.1f}MB exceeds target 12.4MB"
        
        logger.info(f"âœ… Memory usage test: {memory_delta:.1f}MB (target: â‰¤12.4MB)")
    
    def test_accuracy_validation(self, neurocluster_instance):
        """Test algorithm accuracy against known patterns"""
        nc = neurocluster_instance
        
        # Generate test data with known patterns
        test_cases = []
        
        # Bull market pattern
        bull_prices = np.linspace(100, 120, 100) + np.random.normal(0, 0.5, 100)
        for i, price in enumerate(bull_prices):
            market_data = MarketData(
                symbol="TEST",
                asset_type=AssetType.STOCK,
                price=price,
                change=price - (bull_prices[i-1] if i > 0 else price),
                change_percent=1.0 if i > 0 else 0.0,
                volume=1000000,
                timestamp=datetime.now() + timedelta(minutes=i),
                rsi=60 + np.random.uniform(-10, 10),
                volatility=10.0
            )
            test_cases.append((market_data, RegimeType.BULL))
        
        # Bear market pattern
        bear_prices = np.linspace(120, 100, 100) + np.random.normal(0, 0.5, 100)
        for i, price in enumerate(bear_prices):
            market_data = MarketData(
                symbol="TEST",
                asset_type=AssetType.STOCK,
                price=price,
                change=price - (bear_prices[i-1] if i > 0 else price),
                change_percent=-1.0 if i > 0 else 0.0,
                volume=1000000,
                timestamp=datetime.now() + timedelta(minutes=i+100),
                rsi=40 + np.random.uniform(-10, 10),
                volatility=10.0
            )
            test_cases.append((market_data, RegimeType.BEAR))
        
        # Sideways market pattern
        sideways_prices = 110 + np.random.normal(0, 2, 100)
        for i, price in enumerate(sideways_prices):
            market_data = MarketData(
                symbol="TEST",
                asset_type=AssetType.STOCK,
                price=price,
                change=0.0,
                change_percent=0.0,
                volume=1000000,
                timestamp=datetime.now() + timedelta(minutes=i+200),
                rsi=50 + np.random.uniform(-5, 5),
                volatility=5.0
            )
            test_cases.append((market_data, RegimeType.SIDEWAYS))
        
        # Process test cases and measure accuracy
        correct_predictions = 0
        total_predictions = len(test_cases)
        
        for market_data, expected_regime in test_cases:
            result = nc.process_datapoint(market_data)
            if result.regime == expected_regime:
                correct_predictions += 1
        
        accuracy = (correct_predictions / total_predictions) * 100
        
        # Validate accuracy
        assert accuracy >= 85.0, f"Accuracy {accuracy:.1f}% below minimum threshold of 85%"
        
        # Target accuracy is 94.7%, but we allow some tolerance for test data
        target_accuracy = 94.7
        if accuracy >= target_accuracy:
            logger.info(f"âœ… Accuracy test: {accuracy:.1f}% (exceeds target {target_accuracy}%)")
        else:
            logger.warning(f"âš ï¸ Accuracy test: {accuracy:.1f}% (target: {target_accuracy}%)")

# ==================== REGIME DETECTION TESTS ====================

class TestRegimeDetection:
    """Test regime detection functionality"""
    
    def test_regime_detection_stability(self, neurocluster_instance):
        """Test regime detection stability over time"""
        nc = neurocluster_instance
        
        # Generate stable trending data
        prices = np.linspace(100, 110, 50) + np.random.normal(0, 0.1, 50)
        regime_results = []
        
        for i, price in enumerate(prices):
            market_data = MarketData(
                symbol="STABLE_TEST",
                asset_type=AssetType.STOCK,
                price=price,
                change=price - (prices[i-1] if i > 0 else price),
                change_percent=1.0 if i > 0 else 0.0,
                volume=1000000,
                timestamp=datetime.now() + timedelta(minutes=i),
                rsi=55 + np.random.uniform(-5, 5),
                volatility=8.0
            )
            
            result = nc.process_datapoint(market_data)
            regime_results.append(result.regime)
        
        # Check regime consistency (should be mostly BULL for uptrend)
        bull_count = sum(1 for regime in regime_results[-20:] if regime == RegimeType.BULL)
        consistency_ratio = bull_count / 20
        
        assert consistency_ratio >= 0.7, f"Regime detection too unstable: {consistency_ratio:.1%} consistency"
        
        logger.info(f"âœ… Regime stability test: {consistency_ratio:.1%} consistency")
    
    def test_regime_transition_detection(self, neurocluster_instance):
        """Test detection of regime transitions"""
        nc = neurocluster_instance
        
        # Create data with clear regime transition
        bull_phase = np.linspace(100, 120, 30)  # Bull market
        transition_phase = np.linspace(120, 118, 10)  # Transition
        bear_phase = np.linspace(118, 100, 30)  # Bear market
        
        all_prices = np.concatenate([bull_phase, transition_phase, bear_phase])
        regimes = []
        
        for i, price in enumerate(all_prices):
            market_data = MarketData(
                symbol="TRANSITION_TEST",
                asset_type=AssetType.STOCK,
                price=price,
                change=price - (all_prices[i-1] if i > 0 else price),
                change_percent=((price - all_prices[i-1]) / all_prices[i-1] * 100) if i > 0 else 0,
                volume=1000000,
                timestamp=datetime.now() + timedelta(minutes=i),
                rsi=70 - i if i < 30 else 30 + (i-30),  # RSI transition
                volatility=10.0 + (i * 0.2)  # Increasing volatility
            )
            
            result = nc.process_datapoint(market_data)
            regimes.append(result.regime)
        
        # Check for regime transition
        early_regimes = regimes[20:30]  # Late bull phase
        late_regimes = regimes[50:60]   # Late bear phase
        
        bull_in_early = sum(1 for r in early_regimes if r == RegimeType.BULL) > 5
        bear_in_late = sum(1 for r in late_regimes if r == RegimeType.BEAR) > 5
        
        assert bull_in_early, "Failed to detect bull regime in early phase"
        assert bear_in_late, "Failed to detect bear regime in late phase"
        
        logger.info("âœ… Regime transition detection test passed")

# ==================== FEATURE EXTRACTION TESTS ====================

class TestFeatureExtraction:
    """Test feature extraction functionality"""
    
    def test_technical_indicator_extraction(self, neurocluster_instance, sample_market_data):
        """Test extraction of technical indicators"""
        nc = neurocluster_instance
        
        # Test with sample data
        for _, row in sample_market_data.head(10).iterrows():
            market_data = MarketData(
                symbol="FEATURE_TEST",
                asset_type=AssetType.STOCK,
                price=row['close'],
                change=row['close'] - row['open'],
                change_percent=((row['close'] - row['open']) / row['open']) * 100,
                volume=int(row['volume']),
                timestamp=row.name
            )
            
            # Extract features
            features = nc.feature_extractor.extract_features(market_data)
            
            # Validate feature extraction
            assert features is not None
            assert len(features) > 0
            assert all(isinstance(f, (int, float)) for f in features)
            assert all(not np.isnan(f) for f in features)
            assert all(not np.isinf(f) for f in features)
        
        logger.info("âœ… Technical indicator extraction test passed")
    
    def test_feature_consistency(self, neurocluster_instance):
        """Test feature extraction consistency"""
        nc = neurocluster_instance
        
        # Create identical market data
        market_data = MarketData(
            symbol="CONSISTENCY_TEST",
            asset_type=AssetType.STOCK,
            price=150.0,
            change=1.5,
            change_percent=1.0,
            volume=1000000,
            timestamp=datetime.now(),
            rsi=65.0,
            macd=0.5,
            volatility=15.0
        )
        
        # Extract features multiple times
        features_1 = nc.feature_extractor.extract_features(market_data)
        features_2 = nc.feature_extractor.extract_features(market_data)
        features_3 = nc.feature_extractor.extract_features(market_data)
        
        # Check consistency
        assert np.allclose(features_1, features_2, rtol=1e-10)
        assert np.allclose(features_2, features_3, rtol=1e-10)
        
        logger.info("âœ… Feature extraction consistency test passed")

# ==================== PATTERN RECOGNITION TESTS ====================

class TestPatternRecognition:
    """Test pattern recognition functionality"""
    
    def test_pattern_detection_accuracy(self, neurocluster_instance):
        """Test pattern detection accuracy"""
        nc = neurocluster_instance
        
        # Generate known patterns
        
        # Head and Shoulders pattern
        hs_pattern = [100, 105, 103, 108, 106, 104, 102, 100, 98]
        
        # Double Top pattern  
        dt_pattern = [100, 105, 100, 95, 100, 105, 100, 95, 90]
        
        # Triangle pattern
        triangle_pattern = [100, 99, 101, 98, 102, 97, 103, 96, 104]
        
        patterns_data = [
            (hs_pattern, "head_and_shoulders"),
            (dt_pattern, "double_top"),
            (triangle_pattern, "triangle")
        ]
        
        detected_patterns = 0
        
        for pattern_prices, pattern_name in patterns_data:
            market_data_list = []
            
            for i, price in enumerate(pattern_prices):
                market_data = MarketData(
                    symbol="PATTERN_TEST",
                    asset_type=AssetType.STOCK,
                    price=price,
                    change=price - (pattern_prices[i-1] if i > 0 else price),
                    change_percent=1.0 if i > 0 else 0.0,
                    volume=1000000,
                    timestamp=datetime.now() + timedelta(minutes=i),
                    volatility=8.0
                )
                market_data_list.append(market_data)
            
            # Detect patterns
            patterns = nc.pattern_recognition.detect_patterns(market_data_list)
            
            # Check if expected pattern was detected
            if any(pattern_name in str(p).lower() for p in patterns):
                detected_patterns += 1
        
        # We expect at least some pattern detection capability
        pattern_accuracy = detected_patterns / len(patterns_data)
        
        # Allow for some tolerance in pattern detection
        assert pattern_accuracy >= 0.3, f"Pattern detection accuracy too low: {pattern_accuracy:.1%}"
        
        logger.info(f"âœ… Pattern detection test: {pattern_accuracy:.1%} accuracy")

# ==================== PERFORMANCE BENCHMARKS ====================

class TestPerformanceBenchmarks:
    """Comprehensive performance benchmarks"""
    
    @benchmark(threshold=1.0)
    def test_bulk_processing_performance(self, neurocluster_instance):
        """Test performance with large datasets"""
        nc = neurocluster_instance
        
        # Generate large dataset (simulating real-time processing load)
        dataset_size = 10000
        market_data_list = []
        
        for i in range(dataset_size):
            price = 100 + np.sin(i * 0.01) * 10 + np.random.normal(0, 1)
            market_data = MarketData(
                symbol="BULK_TEST",
                asset_type=AssetType.STOCK,
                price=price,
                change=np.random.normal(0, 1),
                change_percent=np.random.normal(0, 2),
                volume=np.random.randint(500000, 2000000),
                timestamp=datetime.now() + timedelta(seconds=i),
                rsi=np.random.uniform(30, 70),
                volatility=np.random.uniform(10, 25)
            )
            market_data_list.append(market_data)
        
        # Measure processing performance
        start_time = time.perf_counter()
        results = nc.process_batch(market_data_list)
        end_time = time.perf_counter()
        
        total_time = end_time - start_time
        avg_time_per_point = (total_time * 1000) / dataset_size  # ms per point
        throughput = dataset_size / total_time  # points per second
        
        # Validate performance
        assert avg_time_per_point <= 0.045, f"Average processing time {avg_time_per_point:.3f}ms exceeds target"
        assert throughput >= 20000, f"Throughput {throughput:.0f} points/sec below target of 20,000"
        
        # Validate efficiency
        final_efficiency = nc.get_current_efficiency()
        assert final_efficiency >= 99.0, f"Final efficiency {final_efficiency:.2f}% below target"
        
        logger.info(f"âœ… Bulk processing: {avg_time_per_point:.3f}ms/point, "
                   f"{throughput:.0f} points/sec, {final_efficiency:.2f}% efficiency")
    
    def test_memory_leak_detection(self, neurocluster_instance):
        """Test for memory leaks during extended processing"""
        nc = neurocluster_instance
        process = psutil.Process()
        
        # Baseline memory
        memory_baseline = process.memory_info().rss / 1024 / 1024  # MB
        
        # Process data in multiple cycles to check for memory leaks
        cycles = 10
        points_per_cycle = 1000
        
        for cycle in range(cycles):
            market_data_list = []
            
            for i in range(points_per_cycle):
                price = 100 + np.random.normal(0, 5)
                market_data = MarketData(
                    symbol="MEMORY_TEST",
                    asset_type=AssetType.STOCK,
                    price=price,
                    change=np.random.normal(0, 1),
                    change_percent=np.random.normal(0, 2),
                    volume=np.random.randint(500000, 2000000),
                    timestamp=datetime.now() + timedelta(seconds=i),
                    rsi=np.random.uniform(30, 70),
                    volatility=np.random.uniform(10, 25)
                )
                market_data_list.append(market_data)
            
            # Process cycle
            results = nc.process_batch(market_data_list)
            
            # Clear references to help garbage collection
            del market_data_list
            del results
        
        # Final memory measurement
        memory_final = process.memory_info().rss / 1024 / 1024  # MB
        memory_growth = memory_final - memory_baseline
        
        # Allow some memory growth but detect significant leaks
        assert memory_growth <= 5.0, f"Potential memory leak detected: {memory_growth:.1f}MB growth"
        
        logger.info(f"âœ… Memory leak test: {memory_growth:.1f}MB growth over {cycles} cycles")

# ==================== INTEGRATION TESTS ====================

class TestNeuroClusterIntegration:
    """Integration tests for NeuroCluster with other components"""
    
    def test_multi_asset_processing(self, neurocluster_instance, multi_asset_data):
        """Test processing multiple asset types"""
        nc = neurocluster_instance
        
        results_by_asset = {}
        
        for symbol, asset_info in multi_asset_data.items():
            data_df = asset_info['data']
            asset_type = asset_info['asset_type']
            
            results = []
            for _, row in data_df.head(100).iterrows():
                market_data = MarketData(
                    symbol=symbol,
                    asset_type=asset_type,
                    price=row['close'],
                    change=row['close'] - row['open'],
                    change_percent=((row['close'] - row['open']) / row['open']) * 100,
                    volume=int(row['volume']),
                    timestamp=row.name,
                    volatility=np.random.uniform(10, 25)
                )
                
                result = nc.process_datapoint(market_data)
                results.append(result)
            
            results_by_asset[symbol] = results
        
        # Validate results for each asset
        for symbol, results in results_by_asset.items():
            assert len(results) == 100
            assert all(r is not None for r in results)
            assert all(0 <= r.confidence <= 100 for r in results)
            
            # Check regime distribution (should have some variety)
            regimes = [r.regime for r in results]
            unique_regimes = set(regimes)
            assert len(unique_regimes) >= 1, f"No regime variety detected for {symbol}"
        
        logger.info(f"âœ… Multi-asset processing test: {len(multi_asset_data)} assets processed")
    
    def test_real_time_simulation(self, neurocluster_instance):
        """Simulate real-time processing"""
        nc = neurocluster_instance
        
        # Simulate real-time data stream
        processing_times = []
        total_points = 1000
        
        for i in range(total_points):
            # Generate realistic market data
            price = 100 + np.sin(i * 0.01) * 5 + np.random.normal(0, 0.5)
            
            market_data = MarketData(
                symbol="REALTIME_TEST",
                asset_type=AssetType.STOCK,
                price=price,
                change=np.random.normal(0, 0.5),
                change_percent=np.random.normal(0, 1),
                volume=np.random.randint(100000, 1000000),
                timestamp=datetime.now(),
                rsi=50 + np.random.normal(0, 10),
                volatility=15 + np.random.normal(0, 3)
            )
            
            # Measure processing time for each point
            start_time = time.perf_counter()
            result = nc.process_datapoint(market_data)
            end_time = time.perf_counter()
            
            processing_time = (end_time - start_time) * 1000  # ms
            processing_times.append(processing_time)
            
            # Validate result
            assert result is not None
            assert 0 <= result.confidence <= 100
        
        # Analyze real-time performance
        avg_processing_time = np.mean(processing_times)
        max_processing_time = np.max(processing_times)
        percentile_95 = np.percentile(processing_times, 95)
        
        # Validate real-time performance requirements
        assert avg_processing_time <= 0.045, f"Average processing time {avg_processing_time:.3f}ms exceeds target"
        assert percentile_95 <= 0.1, f"95th percentile {percentile_95:.3f}ms too high for real-time processing"
        
        logger.info(f"âœ… Real-time simulation: avg={avg_processing_time:.3f}ms, "
                   f"max={max_processing_time:.3f}ms, 95%={percentile_95:.3f}ms")

# ==================== ERROR HANDLING TESTS ====================

class TestErrorHandling:
    """Test error handling and edge cases"""
    
    def test_invalid_input_handling(self, neurocluster_instance):
        """Test handling of invalid inputs"""
        nc = neurocluster_instance
        
        # Test with None input
        with pytest.raises((ValueError, TypeError)):
            nc.process_datapoint(None)
        
        # Test with invalid price
        invalid_market_data = MarketData(
            symbol="INVALID_TEST",
            asset_type=AssetType.STOCK,
            price=-100.0,  # Invalid negative price
            change=0.0,
            change_percent=0.0,
            volume=1000000,
            timestamp=datetime.now()
        )
        
        # Should handle gracefully or raise appropriate error
        try:
            result = nc.process_datapoint(invalid_market_data)
            # If it processes, confidence should be low or regime should be unknown
            if result:
                assert result.confidence <= 50 or result.regime == RegimeType.SIDEWAYS
        except (ValueError, TypeError):
            pass  # Expected behavior for invalid input
        
        logger.info("âœ… Invalid input handling test passed")
    
    def test_extreme_values_handling(self, neurocluster_instance):
        """Test handling of extreme market values"""
        nc = neurocluster_instance
        
        extreme_cases = [
            # Very high price
            MarketData("EXTREME_HIGH", AssetType.STOCK, 1000000.0, 0, 0, 1000000, datetime.now()),
            # Very low price
            MarketData("EXTREME_LOW", AssetType.STOCK, 0.001, 0, 0, 1000000, datetime.now()),
            # High volatility
            MarketData("HIGH_VOL", AssetType.STOCK, 100.0, 50, 50, 1000000, datetime.now(), volatility=100.0),
            # Zero volume
            MarketData("ZERO_VOL", AssetType.STOCK, 100.0, 0, 0, 0, datetime.now()),
        ]
        
        for market_data in extreme_cases:
            try:
                result = nc.process_datapoint(market_data)
                # Should produce valid result even for extreme values
                if result:
                    assert 0 <= result.confidence <= 100
                    assert result.regime in RegimeType
            except Exception as e:
                # Log but don't fail - extreme values might legitimately cause issues
                logger.warning(f"Extreme value handling: {e}")
        
        logger.info("âœ… Extreme values handling test completed")

# ==================== TEST RUNNER ====================

if __name__ == "__main__":
    # Set up test environment
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent))
    
    # Run specific test classes
    test_classes = [
        TestNeuroClusterCore,
        TestRegimeDetection,
        TestFeatureExtraction,
        TestPatternRecognition,
        TestPerformanceBenchmarks,
        TestNeuroClusterIntegration,
        TestErrorHandling
    ]
    
    print("ðŸ§ª Running NeuroCluster Elite Core Algorithm Tests")
    print("=" * 60)
    
    # Run tests with pytest
    exit_code = pytest.main([
        __file__,
        "-v",
        "--tb=short",
        "-x"  # Stop on first failure
    ])
    
    # Print performance summary
    perf_summary = PERFORMANCE_MONITOR.get_summary()
    print(f"\nðŸ“Š Performance Summary:")
    print(f"   Total tests: {perf_summary.get('total_tests', 0)}")
    print(f"   Avg duration: {perf_summary.get('avg_duration', 0):.3f}s")
    print(f"   Max duration: {perf_summary.get('max_duration', 0):.3f}s")
    print(f"   Tests over threshold: {perf_summary.get('tests_over_threshold', 0)}")
    
    sys.exit(exit_code)